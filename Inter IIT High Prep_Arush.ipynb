{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb41d5a-80fa-4a5e-abb3-82bc576778c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.4.5\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "Requirement already satisfied: six in c:\\users\\samad\\anaconda3\\lib\\site-packages (from nltk==3.4.5) (1.16.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=32a4066dfaa6e6562451832e9d3e05cc21c4c89df580af1fb0d5446d09b42257\n",
      "  Stored in directory: c:\\users\\samad\\appdata\\local\\pip\\cache\\wheels\\04\\32\\57\\69e42ad50941013def31e288c6e06bb569442dd993a123cb76\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.4.5\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samad\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\samad\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\samad\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\samad\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.4.5\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8868e69-b0e9-4535-93ac-32208f546097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "#for pre processing\n",
    "import csv,numpy \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f5499-d797-4d85-ae78-3abf90999cbc",
   "metadata": {},
   "source": [
    "Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd60cedf-ab25-4e9c-915d-095cf5daa5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samad\\AppData\\Local\\Temp\\ipykernel_14984\\2845631101.py:19: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purpos', 'preprocess', 'text', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# column index of the column we want to read\n",
    "column_index = 2\n",
    "\n",
    "# list to store the column values\n",
    "column_values = []\n",
    "\n",
    "# open the CSV file in read mode\n",
    "'''\n",
    "with open('/content/drive/MyDrive/DevRev/train_data.csv', 'r') as csv_file:\n",
    "    # creating a CSV reader object\n",
    "    reader = csv.reader(csv_file)\n",
    "    \n",
    "    # iterating over the rows in the CSV file\n",
    "    for row in reader:\n",
    "        # appending the value at the specified column index to the list\n",
    "        column_values.append(row[column_index])\n",
    "'''\n",
    "path = 'train_data.csv'\n",
    "data = pd.read_csv(path)\n",
    "for para in data['Paragraph']:\n",
    "  column_values.append(para)\n",
    "\n",
    "# column_values now contains the values from the specified column\n",
    "\n",
    "# list of paragraphs\n",
    "paragraphs = column_values\n",
    "\n",
    "# question\n",
    "question = \"What is the purpose of preprocessing text data?\"\n",
    "\n",
    "# lowercase the paragraphs and question\n",
    "paragraphs = [p.lower() for p in paragraphs]\n",
    "question = question.lower()\n",
    "\n",
    "# removing stop words and punctuation from the paragraphs and question\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(['.', ',', ':', ';', '?'])\n",
    "\n",
    "filtered_paragraphs = []\n",
    "for p in paragraphs:\n",
    "    filtered_p = []\n",
    "    for w in word_tokenize(p):\n",
    "        if w not in stop_words and w not in punctuation:\n",
    "            filtered_p.append(w)\n",
    "    filtered_paragraphs.append(filtered_p)\n",
    "\n",
    "filtered_question = []\n",
    "for w in word_tokenize(question):\n",
    "    if w not in stop_words and w not in punctuation:\n",
    "        filtered_question.append(w)\n",
    "\n",
    "# stem the paragraphs and question\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_paragraphs = []\n",
    "for p in filtered_paragraphs:\n",
    "    stemmed_p = []\n",
    "    for w in p:\n",
    "        stemmed_p.append(stemmer.stem(w))\n",
    "    stemmed_paragraphs.append(stemmed_p)\n",
    "\n",
    "stemmed_question = []\n",
    "for w in filtered_question:\n",
    "    stemmed_question.append(stemmer.stem(w))\n",
    "\n",
    "# create a list of tokens for the paragraphs and question\n",
    "tokens_p = []\n",
    "tokens_q = []\n",
    "for p in stemmed_paragraphs:\n",
    "    tokens_p.append(p)\n",
    "tokens_q.extend(stemmed_question)\n",
    "print(tokens_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfa06a7-cb08-4308-8426-d6ab76c738e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samad\\AppData\\Local\\Temp\\ipykernel_8952\\2320214636.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "path = 'train_data.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f252c69-30ab-40ba-ac0e-f1a7080bfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQ = ''\n",
    "# for tok in tokens_q[0]:\n",
    "#     TQ += tok + ' '\n",
    "questions = []\n",
    "for q in data['Question']:\n",
    "    if q not in questions:\n",
    "          questions.append(q)\n",
    "themes = []\n",
    "for t in data['Theme']:\n",
    "    if t not in themes:\n",
    "        themes.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb0b33-ea99-4bff-b7ca-29f77b6dac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraphs = []\n",
    "for para in data['Paragraph']:\n",
    "    if para not in Paragraphs:\n",
    "        Paragraphs.append(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3daf1834-d06b-488d-bb9e-da186eff1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = data['Paragraph'][data['Theme'] == themes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "020bdf9a-f983-4fa7-930c-c14f3f6d627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paras = []\n",
    "for para in P:\n",
    "    if para not in Paras:\n",
    "        Paras.append(para)\n",
    "Paras.append(questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefa22d-3d51-4c3a-8170-8bdb17c3fa61",
   "metadata": {},
   "source": [
    "Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de740c79-01c9-46ca-a5eb-376374ce9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aabb6715-c004-4095-9c47-bd7c125c4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Model = AutoModel.from_pretrained(model_name)\n",
    "tokens = {'input_ids' : [], 'attention_mask' : []}\n",
    "for el in Paras:\n",
    "    new_tokens = Tokenizer.encode_plus(el, max_length = 128,\n",
    "                                       truncation = True, padding = 'max_length',\n",
    "                                       return_tensors = 'pt') \n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "outputs = Model(**tokens)\n",
    "embeddings = outputs.last_hidden_state\n",
    "attention = tokens['attention_mask']\n",
    "mask = attention.unsqueeze(-1).expand(embeddings.shape).float()\n",
    "mask_embeddings = embeddings*mask\n",
    "summed = torch.sum(mask_embeddings, 1)\n",
    "counts = torch.clamp(mask.sum(1), min = 1e-9)\n",
    "mean_pooled = (summed/counts).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "193c283b-e00a-4198-bea2-2d842f104c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6226311 , 0.62663156, 0.4643542 , 0.6044539 , 0.5350107 ,\n",
       "        0.5302644 , 0.4876517 , 0.47589162, 0.5421442 , 0.42167842,\n",
       "        0.52171105, 0.64418894, 0.47883362, 0.52977973, 0.5128255 ,\n",
       "        0.5041628 , 0.53310674, 0.59048355, 0.52646327, 0.47564873,\n",
       "        0.5249543 , 0.5127542 , 0.5064064 , 0.54416806, 0.5075203 ,\n",
       "        0.52174455, 0.5882031 , 0.47375882, 0.53860724, 0.45436174,\n",
       "        0.5575513 , 0.4690459 , 0.42037868, 0.41298923, 0.4877195 ,\n",
       "        0.5286655 , 0.56423485, 0.5745464 , 0.60030746, 0.59218776,\n",
       "        0.42867064, 0.5934118 , 0.44822526, 0.6509285 , 0.4580393 ,\n",
       "        0.462371  , 0.52888644, 0.47936013, 0.47673216, 0.46733046,\n",
       "        0.5968343 , 0.61360645, 0.40107253, 0.40395582, 0.49821523,\n",
       "        0.4034803 , 0.5708873 , 0.506723  , 0.48446858, 0.49564862,\n",
       "        0.5106265 , 0.47472575, 0.4982315 , 0.38249168, 0.48430756,\n",
       "        0.45806777]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity([mean_pooled[-1]], mean_pooled[:len(mean_pooled)-1])\n",
    "similarity[0].tolist().index(max(similarity[0]))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71dcfbe3-ef79-46b2-9640-dda2a68c585c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When did Beyonce leave Destiny's Child and become a solo singer?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paras[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9ab150f-6848-4de2-ac68-f14fdb107523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Described as being \"sexy, seductive and provocative\" when performing on stage, Beyoncé has said that she originally created the alter ego \"Sasha Fierce\" to keep that stage persona separate from who she really is. She described Sasha as being \"too aggressive, too strong, too sassy [and] too sexy\", stating, \"I\\'m not like her in real life at all.\" Sasha was conceived during the making of \"Crazy in Love\", and Beyoncé introduced her with the release of her 2008 album I Am... Sasha Fierce. In February 2010, she announced in an interview with Allure magazine that she was comfortable enough with herself to no longer need Sasha Fierce. However, Beyoncé announced in May 2012 that she would bring her back for her Revel Presents: Beyoncé Live shows later that month.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paras[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2659571-fae2-401e-8d3c-35486dcca108",
   "metadata": {},
   "source": [
    "Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ffd6def-5b53-44ff-a7c4-f35d95340a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_answer(question, paragraphs, theme):\n",
    "    # Filter the paragraphs to only include those with the correct theme\n",
    "    theme_paragraphs = [p['text'] for p in paragraphs if p['theme'] == theme]\n",
    "    # cummulate = ' '.join(theme_paragraphs)\n",
    "    # for trial in theme_paragraphs:\n",
    "    #     encoding = tokenizer.encode_plus(text=question,text_pair= trial)\n",
    "    #     inputs = encoding['input_ids']  #Token embeddings\n",
    "    #     sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "    #     tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "    #     start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "    #     start_index = torch.argmax(start_scores)\n",
    "    #     end_index = torch.argmax(end_scores)\n",
    "    #     final_answer = ' '.join(tokens[end_index:start_index+1])\n",
    "    #     print(final_answer)\n",
    "    # Create a TfidfVectorizer to convert the paragraphs into numerical vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(theme_paragraphs)\n",
    "    \n",
    "    # Convert the question into a numerical vector\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    \n",
    "    # Calculate the cosine similarity between the question and each paragraph\n",
    "    similarity = cosine_similarity(question_vector, X)\n",
    "    \n",
    "    # Find the paragraph with the highest similarity score\n",
    "    max_similarity_index = similarity.argmax()\n",
    "    max_similarity_score = similarity[0, max_similarity_index]\n",
    "    \n",
    "    # If the maximum similarity score is above a certain threshold, return the paragraph\n",
    "    if max_similarity_score > 0.12:\n",
    "        return theme_paragraphs[max_similarity_index]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e94d97b7-1392-45c7-bc1e-ebeef766b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumm_para = ''\n",
    "for c in Paragraphs:\n",
    "    cumm_para += c+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a37302c4-33a6-4f14-beca-370db7f9f01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08815a8152bc4e03ac54e00ce8421e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deae2236261b43a4bacbcf0aae8f88a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cd3311047243a18598c8b5a67ff25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Initialize the BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d41d0ae8-fe40-4838-b251-c5dcdd84443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2382008 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "question = questions[0]\n",
    "encoding = tokenizer.encode_plus(text=question,text_pair= cumm_para)\n",
    "inputs = encoding['input_ids']  #Token embeddings\n",
    "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8424bf0-a441-499c-925b-85d0fe9e8c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7317528576 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m start_scores, end_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m start_scores\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1014\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1022\u001b[0m     embedding_output,\n\u001b[0;32m   1023\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m--> 232\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7317528576 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "start_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff5419-f6ce-478d-9037-80515c27d094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e0a39-76d3-44ae-b574-1585862d4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = questions[0]\n",
    "# paragraphs = [\n",
    "#     {'text': 'Paris is the capital of France', 'theme': 'Geography'},\n",
    "#     {'text': 'Lyon is a city in France', 'theme': 'Geography'},\n",
    "#     {'text': 'The Pythagorean theorem states that a^2 + b^2 = c^2', 'theme': 'Mathematics'},\n",
    "# ]\n",
    "\n",
    "# # Predict the answer to the question\n",
    "# answer = predict_answer(question, paragraphs, 'Geography')\n",
    "\n",
    "# # Print the answer\n",
    "# if answer:\n",
    "#     print(answer)\n",
    "# else:\n",
    "#     print(\"No suitable paragraph found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8cdf22-8b36-45c1-8510-7b772fd38ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = []\n",
    "Themes = []\n",
    "s = 0\n",
    "for element in Paragraphs:\n",
    "    \n",
    "    Themes.append(data['Theme'][data.loc[data['Paragraph'] == element]['Theme'].index[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "db76df25-48ae-4644-886f-d504934b3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution = []\n",
    "for element in questions:\n",
    "    Solution.append(data['Answer_text'][data.loc[data['Question'] == element]['Answer_text'].index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ff3dd17-8ad3-4ce7-9e02-df311a790a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frédéric_Chopin'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Theme'][data.loc[data['Paragraph'] == Paragraphs[100]]['Theme'].index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afe2307-9331-48e1-8ddb-598c864dfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Themes)):\n",
    "    PT.append({'text': Paragraphs[i], 'theme': Themes[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99f4d51a-cc1b-45cb-a113-aeeb5a5317be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "\n",
      "\n",
      "\n",
      "1990\n",
      "1995\n",
      "1999\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[SEP]\n",
      "[SEP]\n",
      "2010\n",
      "\n",
      "2011\n",
      "2010\n",
      "\n",
      "2013\n",
      "\n",
      "\n",
      "2015\n",
      "\n",
      "\n",
      "2011\n",
      "\n",
      "2009\n",
      "\n",
      "2015\n",
      "\n",
      "\n",
      "[SEP]\n",
      "90s\n",
      "\n",
      "1991\n",
      "2006\n",
      "2006\n",
      "\n",
      "2006\n",
      "[SEP] beyonce\n",
      "2008\n",
      "2000s\n",
      "2012\n",
      "2002\n",
      "[SEP] the bey hive is the name given to beyonce\n",
      "2011\n",
      "2007 , that he thinks race plays a role in many of these criticisms , saying white celebrities who dress similarly do not attract as many comments . in 2008\n",
      "\n",
      "2012\n",
      "2013\n",
      "2009 , the observer named her the artist of the decade and billboard named her the top female artist and top radio songs artist of the decade . in 2010 , billboard named her in their \" top 50 r & b / hip - hop artists of the past 25 years \" list at number 15 . in 2012 vh1 ranked her third on their list of the \" 100 greatest women in music \" . beyonce was the first female artist to be honored with the international artist award at the american music awards . she has also received the legend award at the 2008\n",
      "2010\n",
      "2002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2005\n",
      "2005\n",
      "\n",
      "2005\n",
      "2010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = questions[0]\n",
    "answer = predict_answer(question, PT, 'Beyoncé')\n",
    "if answer:\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"No suitable paragraph found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d3e4318-ead7-47df-900b-7fd56b2d8f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Who managed the Destiny's Child group?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238ca30a-f9a5-4411-9c4b-888fb16e5473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The group changed their name to Destiny\\'s Child in 1996, based upon a passage in the Book of Isaiah. In 1997, Destiny\\'s Child released their major label debut song \"Killing Time\" on the soundtrack to the 1997 film, Men in Black. The following year, the group released their self-titled debut album, scoring their first major hit \"No, No, No\". The album established the group as a viable act in the music industry, with moderate sales and winning the group three Soul Train Lady of Soul Awards for Best R&B/Soul Album of the Year, Best R&B/Soul or Rap New Artist, and Best R&B/Soul Single for \"No, No, No\". The group released their multi-platinum second album The Writing\\'s on the Wall in 1999. The record features some of the group\\'s most widely known songs such as \"Bills, Bills, Bills\", the group\\'s first number-one single, \"Jumpin\\' Jumpin\\'\" and \"Say My Name\", which became their most successful song at the time, and would remain one of their signature songs. \"Say My Name\" won the Best R&B Performance by a Duo or Group with Vocals and the Best R&B Song at the 43rd Annual Grammy Awards. The Writing\\'s on the Wall sold more than eight million copies worldwide. During this time, Beyoncé recorded a duet with Marc Nelson, an original member of Boyz II Men, on the song \"After All Is Said and Done\" for the soundtrack to the 1999 film, The Best Man.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8799d5fe-b9de-407e-9cae-73247fd1123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "# Initialize the BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', return_dict=False)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9a4feb-2d35-4991-b265-fa9aa22c9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(text=question,text_pair= answer)\n",
    "inputs = encoding['input_ids']  #Token embeddings\n",
    "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05b36ecf-8507-472a-ab6c-24ebd3cf7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cb77d2e-0917-48f0-8d01-f8b7e3b46024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7706, -7.2730, -8.2646, -8.3917, -8.0885, -9.0185, -9.0886, -9.1559,\n",
       "         -8.9324, -9.2371, -0.7706, -4.5914, -6.5819, -5.6477, -7.3688, -6.7837,\n",
       "         -7.3916, -5.1371, -8.8219, -8.1138, -7.9400, -6.7756, -3.4717, -7.8041,\n",
       "         -5.5063, -8.5547, -7.1756, -6.4896, -8.2490, -6.7955, -5.5777, -8.5795,\n",
       "         -6.1490, -0.7713, -5.4493, -4.0023, -8.2799, -5.1208, -8.6579, -8.3759,\n",
       "         -8.2101, -6.9559, -7.4337, -4.7180, -7.5110, -6.5353, -7.0225, -6.7830,\n",
       "         -6.1602, -7.9648, -7.9754, -7.1390, -7.1975, -4.8113, -8.5358, -7.6213,\n",
       "         -4.4523, -6.4658, -8.1768, -5.5290, -8.3147, -7.4403, -8.0779, -6.0659,\n",
       "         -7.4882, -7.7166, -8.3190, -6.7098, -7.5801, -7.5903, -7.5324, -6.8471,\n",
       "         -8.8198, -8.1568, -6.1447, -6.8109, -8.0516, -6.6115, -8.0142, -7.3577,\n",
       "         -7.6770, -7.6329, -7.4716, -7.0594, -8.9132, -8.7029, -9.2535, -8.5108,\n",
       "         -8.6886, -7.8206, -6.6940, -7.5430, -6.9247, -7.7752, -8.5381, -8.5796,\n",
       "         -8.3026, -7.6781, -8.9149, -8.6405, -8.0484, -6.7639, -8.2765, -8.5204,\n",
       "         -7.7838, -6.5313, -7.6249, -8.7766, -6.5784, -7.6687, -8.5687, -5.9452,\n",
       "         -2.8534, -6.8654, -7.0375, -8.9772, -7.6845, -8.0661, -8.9829, -7.2696,\n",
       "         -7.8234, -9.0759, -8.8512, -8.9232, -7.9594, -8.0799, -8.6806, -9.0525,\n",
       "         -8.8352, -8.9530, -7.3827, -8.0291, -9.0640, -8.9534, -8.9534, -8.6299,\n",
       "         -8.8786, -8.4546, -8.3147, -8.7260, -9.0397, -8.7217, -7.3822, -8.4143,\n",
       "         -9.2336, -9.1252, -9.0291, -8.7243, -7.9081, -8.5963, -8.1289, -7.8639,\n",
       "         -9.1019, -8.8135, -9.3185, -8.9314, -9.1624, -7.7904, -6.5564, -7.9305,\n",
       "         -8.0550, -7.9451, -6.7715, -8.9713, -8.2761, -7.0980, -7.9399, -7.2764,\n",
       "         -8.0002, -9.1593, -8.9688, -8.8557, -8.9380, -8.8160, -8.0321, -4.6935,\n",
       "         -8.4616, -8.2339, -8.7721, -8.7616, -7.7783, -8.6654, -8.2790, -8.5891,\n",
       "         -8.3745, -8.8169, -8.5783, -8.8234, -8.8068, -8.5138, -8.6325, -8.9956,\n",
       "         -7.8051, -7.7676, -9.0202, -8.7432, -9.2321, -9.0089, -9.2664, -9.1397,\n",
       "         -7.7757, -8.3938, -8.5945, -8.7974, -8.0113, -8.1427, -8.9529, -8.7992,\n",
       "         -8.5962, -9.0177, -7.6441, -7.7333, -8.9159, -9.1707, -8.7703, -9.1275,\n",
       "         -9.3614, -9.0985, -8.7997, -7.8332, -7.9425, -8.9852, -9.1437, -9.0324,\n",
       "         -9.0039, -8.5783, -8.4758, -8.3237, -8.3593, -8.8054, -8.8299, -8.5650,\n",
       "         -8.8243, -9.0111, -8.9528, -8.6414, -8.0181, -8.8352, -8.2921, -8.7212,\n",
       "         -8.5106, -8.4373, -8.8472, -8.9603, -8.1969, -8.5490, -9.0425, -9.2190,\n",
       "         -9.1085, -8.3971, -8.7926, -7.7840, -8.4041, -9.0125, -9.1135, -8.7278,\n",
       "         -8.8473, -8.7574, -7.9912, -8.8757, -8.7793, -8.9198, -9.0145, -8.8534,\n",
       "         -8.5881, -7.8555, -8.6289, -9.0531, -9.1803, -8.8959, -8.6080, -8.1253,\n",
       "         -7.5764, -8.8083, -8.0717, -8.9416, -8.0741, -7.5054, -8.3537, -9.0059,\n",
       "         -8.8038, -8.6470, -8.7768, -8.9381, -8.0786, -8.3224, -8.8981, -8.1389,\n",
       "         -8.8528, -8.8297, -8.4347, -8.6811, -7.7767, -8.2585, -8.6361, -8.7737,\n",
       "         -3.8575, -8.1609, -8.7539, -7.9079, -8.2518, -5.6793, -7.3283, -8.5933,\n",
       "         -7.6146, -7.8378, -7.9454, -8.5094, -5.5754, -8.4108, -8.4982, -8.0657,\n",
       "         -8.9651, -8.5779, -8.8880, -8.5545, -8.3475, -8.3128, -8.9269, -9.1529,\n",
       "         -8.8522, -9.1566, -9.1579, -9.0837, -8.5842, -8.6339, -7.6930, -8.9167,\n",
       "         -8.5242, -6.8164, -8.1756, -8.7966, -7.6984, -8.1693, -8.5041, -9.3232,\n",
       "         -0.7706]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d89132-b587-4ddf-b28a-e9ebf4049408",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = torch.argmax(start_scores)\n",
    "\n",
    "end_index = torch.argmax(end_scores)\n",
    "\n",
    "final_answer = ''.join(tokens[end_index:start_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47316750-0e62-441b-92a5-c4264e1c2777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "770def2e-06c6-48fd-a802-713e0a75386b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "136b35a6-6400-4806-b23d-3e04e6c70608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "beb0dd59-6705-42ff-96a5-0339db21c933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6baf8c2-d4d9-47ce-aad1-17777d2bdc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
